{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. ASL Recongnition using Deep LearningÂ¶\n\nCode Below","metadata":{}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(5) \nimport tensorflow as tf\ntf.set_random_seed(2)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport cv2\n\ntrain_dir = \"../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\"\neval_dir = \"../input/asl-alphabet-test/asl-alphabet-test\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-27T11:41:55.821315Z","iopub.execute_input":"2023-07-27T11:41:55.821609Z","iopub.status.idle":"2023-07-27T11:41:56.960234Z","shell.execute_reply.started":"2023-07-27T11:41:55.821552Z","shell.execute_reply":"2023-07-27T11:41:56.959177Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 2. Loading the data\n\n","metadata":{}},{"cell_type":"code","source":"#Helper function to load images from given directories\ndef load_images(directory):\n    images = []\n    labels = []\n    for idx, label in enumerate(uniq_labels):\n        for file in os.listdir(directory + \"/\" + label):\n            filepath = directory + \"/\" + label + \"/\" + file\n            image = cv2.resize(cv2.imread(filepath), (64, 64))\n            images.append(image)\n            labels.append(idx)\n    images = np.array(images)\n    labels = np.array(labels)\n    return(images, labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:41:56.963563Z","iopub.execute_input":"2023-07-27T11:41:56.963829Z","iopub.status.idle":"2023-07-27T11:41:56.970195Z","shell.execute_reply.started":"2023-07-27T11:41:56.963779Z","shell.execute_reply":"2023-07-27T11:41:56.968856Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import keras\n\nuniq_labels = sorted(os.listdir(train_dir))\nimages, labels = load_images(directory = train_dir)\n\nif uniq_labels == sorted(os.listdir(eval_dir)):\n    X_eval, y_eval = load_images(directory = eval_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Insights of Datasets ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(images, labels, test_size = 0.1, stratify = labels)\n\nn = len(uniq_labels)\ntrain_n = len(X_train)\ntest_n = len(X_test)\n\nprint(\"Total number of symbols: \", n)\nprint(\"Number of training images: \" , train_n)\nprint(\"Number of testing images: \", test_n)\n\neval_n = len(X_eval)\nprint(\"Number of evaluation images: \", eval_n)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.556418Z","iopub.status.idle":"2023-07-27T11:46:23.556892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function will create a grid of 8x4 images and fill 29 images in the first 29 of the 32 spaces.","metadata":{}},{"cell_type":"code","source":"#Helper function to print images\ndef print_images(image_list):\n    n = int(len(image_list) / len(uniq_labels))\n    cols = 8\n    rows = 4\n    fig = plt.figure(figsize = (24, 12))\n\n    for i in range(len(uniq_labels)):\n        ax = plt.subplot(rows, cols, i + 1)\n        plt.imshow(image_list[int(n*i)])\n        plt.title(uniq_labels[i])\n        ax.title.set_fontsize(20)\n        ax.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.557509Z","iopub.status.idle":"2023-07-27T11:46:23.558199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With our helper function now ready, we begin to print the images. The code lines above the print command: \n\n`y_train_in = y_train.argsort()\ny_train = y_train[y_train_in]\nX_train = X_train[y_train_in]` \n\nsort the data according to the symbols, making it easier for us to deal with it, and ensures that we do not run into any mismatches of data image and its symbol.","metadata":{}},{"cell_type":"code","source":"y_train_in = y_train.argsort()\ny_train = y_train[y_train_in]\nX_train = X_train[y_train_in]\n\nprint(\"Training Images: \")\nprint_images(image_list = X_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.559135Z","iopub.status.idle":"2023-07-27T11:46:23.560177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how the training images are all in a similar environment, with a combination of lights and shadows. We will address these issues later.\n\nLet us now print the testing images.","metadata":{}},{"cell_type":"code","source":"y_test_in = y_test.argsort()\ny_test = y_test[y_test_in]\nX_test = X_test[y_test_in]\n\nprint(\"Testing images: \")\nprint_images(image_list = X_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.562165Z","iopub.status.idle":"2023-07-27T11:46:23.562869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluation images: \")\nprint_images(image_list = X_eval)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.563816Z","iopub.status.idle":"2023-07-27T11:46:23.564535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the evaluation images do not, in fact, look at all similar to the training images. They have different hues and vastly different backgrounds. Indeed they are a much better embodiment of 'real-world data'. Therefore these will make a good test for our model's performance on 'real-world' ASL images.\n\n# 4. Preprocessing: One-hot enconding the data\n\n","metadata":{}},{"cell_type":"code","source":"y_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\ny_eval = keras.utils.to_categorical(y_eval)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.565381Z","iopub.status.idle":"2023-07-27T11:46:23.566076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can have a look at one of the labels to see if it is indeed one-hot encoded:","metadata":{}},{"cell_type":"code","source":"print(y_train[0])\nprint(len(y_train[0]))","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.566919Z","iopub.status.idle":"2023-07-27T11:46:23.567614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 5. Preprocessing - Normalize RGB values\n\nNow let us look at how the image data is stored. There are three components for each image - one component each for the Red, Green, and Blue (RGB) channels. The component values are stored as integer numbers in the range 0 to 255, the range that a single 8-bit byte can offer. \n","metadata":{}},{"cell_type":"code","source":"X_train = X_train.astype('float32')/255.0\nX_test = X_test.astype('float32')/255.0\nX_eval = X_eval.astype('float32')/255.0","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.568463Z","iopub.status.idle":"2023-07-27T11:46:23.569159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having loaded, understood and pre-processed our data, it is finally time to turn to the model.\n\n# 5. Define and run the model\n\n\nUsing CNN Here ","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.layers import Flatten, Dense\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Conv2D(filters = 64, kernel_size = 5, padding = 'same', activation = 'relu', \n                 input_shape = (64, 64, 3)))\nmodel.add(Conv2D(filters = 64, kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (4, 4)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(filters = 128 , kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(Conv2D(filters = 128 , kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (4, 4)))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(filters = 256 , kernel_size = 5, padding = 'same', activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(29, activation='softmax'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.570023Z","iopub.status.idle":"2023-07-27T11:46:23.570714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After having defined a neural network in `keras`, the next step is to compile it.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.571568Z","iopub.status.idle":"2023-07-27T11:46:23.572252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're ready to fit it to the training data. The values `acc` will give us the accuracy of the model after each epoch. I will run the model for a total of 5 epochs. Despite our high number of parameters and large dataset, the model runs rather quickly as `keras` is able to engage the GPU in its functioning.","metadata":{}},{"cell_type":"code","source":"hist = model.fit(X_train, y_train, epochs = 5, batch_size = 64)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.573120Z","iopub.status.idle":"2023-07-27T11:46:23.573854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Test the model\n\nWe can now test the model on our testing and evaluation images. We are looking to note the difference between how the model performs with the testing images vs the evaluation or real-world images.","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(x = X_test, y = y_test, verbose = 0)\nprint('Accuracy for test images:', round(score[1]*100, 3), '%')\nscore = model.evaluate(x = X_eval, y = y_eval, verbose = 0)\nprint('Accuracy for evaluation images:', round(score[1]*100, 3), '%')","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.574723Z","iopub.status.idle":"2023-07-27T11:46:23.575398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# 8. Confusion Matrices\n\n","metadata":{}},{"cell_type":"code","source":"#Helper function to plot confusion matrix\ndef plot_confusion_matrix(y, y_pred):\n    y = np.argmax(y, axis = 1)\n    y_pred = np.argmax(y_pred, axis = 1)\n    cm = confusion_matrix(y, y_pred)\n    plt.figure(figsize = (24, 20))\n    ax = plt.subplot()\n    plt.imshow(cm, interpolation = 'nearest', cmap = plt.cm.Purples)\n    plt.colorbar()\n    plt.title(\"Confusion Matrix\")\n    tick_marks = np.arange(len(uniq_labels))\n    plt.xticks(tick_marks, uniq_labels, rotation=45)\n    plt.yticks(tick_marks, uniq_labels)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    ax.title.set_fontsize(20)\n    ax.xaxis.label.set_fontsize(16)\n    ax.yaxis.label.set_fontsize(16)\n    limit = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment = \"center\",color = \"white\" if cm[i, j] > limit else \"black\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.576264Z","iopub.status.idle":"2023-07-27T11:46:23.576973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With our function now ready we can plot the first confusion matrix. This will be the matrix for the testing data, which gave a high accuracy. We expect to find the diagonal elements to have large values with some values distributed in non-diagonal elements. Note that our the matrix is not normalized, and the total number of testing images per label were 300.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ny_test_pred = model.predict(X_test, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_test, y_test_pred)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.577834Z","iopub.status.idle":"2023-07-27T11:46:23.578525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that most of diagonal elements have values 300, and the non-diagonal elements have values 0, indicating that the model labeled most of our data correctly.\n\nNext I will plot the confusion matrix for the evaluation images. Once again, the matrix is not normalized so it is important to note that for every label we had 30 evaluation images.","metadata":{}},{"cell_type":"code","source":"y_eval_pred = model.predict(X_eval, batch_size = 64, verbose = 0)\nplot_confusion_matrix(y_eval, y_eval_pred)","metadata":{"execution":{"iopub.status.busy":"2023-07-27T11:46:23.579374Z","iopub.status.idle":"2023-07-27T11:46:23.580079Z"},"trusted":true},"execution_count":null,"outputs":[]}]}